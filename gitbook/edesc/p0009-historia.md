#### Historia

Ya que en la sección anterior exploramos cómo la estadística se conecta con la filosofía del conocimiento, conviene aclarar algo importante: esta disciplina no nació simplemente porque alguien malvado quiso obligar a estudiantes de Ciencias Sociales a seguir sufriendo con las matemáticas (como si no hubiera sido suficientemente traumático en la educación media). Surgió mucho antes, como una necesidad humana básica. Desde que las sociedades comenzaron a organizarse, también comenzó la necesidad de contar. Cuántos animales se cazaron. Cuánta cebada se almacenó. Cuántas personas componen una tribu. Cuántas herramientas quedan. Qué rutas son más seguras.

Desde los registros agrícolas en tablillas de arcilla en Sumeria hasta los censos organizados en la China imperial, pasando por los catastros egipcios y los documentos fiscales del Imperio Romano, hay una constante: alguien tenía que anotar. Contar era poder, pero también era claridad. Quién vive dónde, qué se debe, qué pasó el año anterior. Incluso en la Biblia aparece esta idea: el Evangelio según Lucas relata que Jesús nació en Belén porque José tuvo que viajar hasta allí para ser censado, como resultado de un decreto imperial. Independiente de si esa historia ocurrió exactamente así, muestra que hasta en la tradición judeocristiana los censos aparecen como parte del ejercicio de gobierno.

Más adelante, ya en sociedades más complejas, ese impulso se sofisticó: se empezó a clasificar, a registrar cambios, a prever demandas. Y comienza a desarrollarse el largo camino que recorre la matemática hasta convertirse en la estadística moderna.

¿De quién es la culpa entonces? ¡Sorpresa! Debemos comenzar con los griegos. Para **Pitágoras** (c. 570–495 a.C.), los números no solo eran útiles: eran la clave del cosmos. Su escuela, una mezcla de religión, ciencia y matemática, recopiló teoremas geométricos fundamentales, como el famoso teorema que lleva su nombre, y otros menos conocidos como la teoría de las proporciones o las propiedades de los polígonos regulares. Para **Euclides** (c. 300 a.C.), la geometría era una manera de entender la lógica del espacio. Su legado, compilado en los *Elementos*, todavía se usa como base de la enseñanza geométrica. Siglos más tarde, matemáticos y filósofos seguirían ese legado. Entre ellos, destaca **Hipatia** (c. 360–415), "la auténtica maestra de los misterios de la filosofía", quien enseñó y desarrolló matemáticas, estudió a Platón y Aristóteles, y atrajo discípulos desde todas partes del Mediterráneo para aprender de sus lecciones en Alejandría. Fue asesinada brutalmente por una turba de fanáticos religiosos, en un crimen que simboliza el paso de una era centrada en la razón a otra donde ésta sería dejada en segundo plano por siglos.

En Asia y en el mundo islámico medieval se tejían otras rutas. Muchas ideas que se silenciaron o pasaron a segundo plano en Europa sobrevivieron y florecieron en Bagdad, Damasco o Córdoba. **Al-Juarismi** (c. 780–850), desde la Casa de la Sabiduría en Bagdad, sistematizó los procedimientos para resolver ecuaciones y dio origen tanto al álgebra como al concepto de "algoritmo" (la palabra viene de su nombre latinizado: *Algoritmi*). Su influencia se expandió por Europa a través de traducciones siglos después. En India y China se desarrollaban métodos astronómicos, técnicas de interpolación, y aproximaciones numéricas que serían fundamento para los cálculos futuros, aunque sus nombres rara vez aparecen en los libros de texto occidentales.

Ya en el Renacimiento, los juegos de azar despertaron una curiosa inquietud entre nobles ociosos y matemáticos perspicaces. ¿Cuál es la probabilidad de ganar en cierto juego? ¿Qué tan justo es un dado? Preguntas aparentemente banales que llevaron a **Blaise Pascal** (1623–1662) y **Pierre de Fermat** (1607–1665) a intercambiar ideas que fundaron sin querer la teoría de la probabilidad. Uno de los problemas clásicos de la época era el “problema de los puntos”, donde se trataba de repartir una apuesta interrumpida entre jugadores de manera justa según sus posibilidades de ganar. Después vendrían **Jacob Bernoulli** (1655–1705) con su *Ars Conjectandi* y **Abraham de Moivre** (1667–1754) con su *Doctrine of Chances*, que acercaron esos conceptos a la idea moderna de distribución probabilística.

En paralelo, también desde Francia, **René Descartes** (1596–1650) que venía dudando sobre su propia existencia,  escribiendo tratados de metafísica y estableciendo fundamentos para la racionalidad moderna, a la vez también propuso el plano cartesiano: esa idea brillante de cruzar dos ejes perpendiculares para ubicar cualquier punto en el espacio. A partir de ahí, emergen estrategias para representar visualmente los datos. Aprovechando esta herramienta, **William Playfair** (1759–1823), un excéntrico economista escocés -y posiblemente un delincuente falsificador de billetes franceses y cuestionable agente secreto británico- inventó el gráfico de líneas, el gráfico de barras y el gráfico circular.

En el siglo XIX, los problemas prácticos se acumulan: los astrónomos quieren saber cuánto error hay en sus observaciones; los censistas, cuán confiables son sus recuentos; los médicos, cuán frecuentes son ciertas enfermedades. Y ahí aparece **Carl Friedrich Gauss** (1777–1855), considerado uno de los genios matemáticos más influyentes de todos los tiempos. Desde joven mostraba una inteligencia fuera de serie. Su aporte más duradero para nuestro contexto fue formalizar la distribución normal: una curva suave y simétrica donde la mayoría de los casos se agrupan cerca del promedio y los extremos se vuelven raros. Esta campana estadística, útil para modelar errores, tiempos, puntajes y decenas de otras cosas, se volvió central. Más adelante, sería adoptada en psicología para modelar rasgos como la inteligencia, marcando el inicio de lo que hoy llamamos **psicometría**.

Ese puente entre estadística y mente humana sería cruzado por uno de sus admiradores: **Francis Galton** (1822–1911). Curioso, excéntrico y bastante obsesivo, intentó medir de todo: tiempos de reacción, fuerza de agarre, rapidez de cálculo, capacidad de juicio. Creía que todo eso podía cuantificarse, e intentó hacerlo. Estableció laboratorios para registrar esas diferencias, inventó formas de visualizar relaciones entre variables, introdujo el concepto de regresión hacia la media, y dejó esbozada la idea de correlación. A su modo, fue uno de los fundadores de la medición sistemática de atributos psicológicos y sociales.

Pero aquí la historia toma un giro oscuro. Galton estaba convencido de que las diferencias entre personas eran en gran parte hereditarias, y propuso que la humanidad podía y debía mejorar... limitando la reproducción de quienes consideraba menos aptos. Así nació el concepto de **eugenesia**, una ideología profundamente nociva que se disfrazó de ciencia durante décadas. Las consecuencias de esa idea -en políticas, discursos y crímenes- no se detuvieron con su muerte. De hecho, aún persisten ecos de ese pensamiento en la obsesión contemporánea por la "inteligencia" medida como esencia individual, o en ideas simplistas sobre meritocracia biológica.

Uno de los discípulos más destacados de Galton fue **Karl Pearson** (1857–1936), que tomó muchas de esas intuiciones y les dio forma matemática. Desarrolló herramientas como la desviación estándar, formalizó el coeficiente de correlación, introdujo el test chi-cuadrado para variables categóricas, y estableció la estadística matemática como disciplina autónoma dentro de las universidades. También fundó revistas, institutos, y dejó una huella técnica profunda. Y sí, también estaba igual de convencido que su mentor de que había diferencias humanas innatas entre razas, clases sociales, géneros y lo que se le cruzara por delante. En algunas cosas, se podría llegar a decir que es otro intelectual pre-nazi. Por otro lado, también apoyó ideas progresistas para su época, como el acceso igualitario a la educación científica. La historia de la ciencia es así: desordenada, ambigua, complicada. Hay que aprender a leerla con honestidad, sin blanquear ni simplificar.

**Ronald Fisher** (1890–1962), muchas veces planteado como padre de la estadística moderna. Desarrolló los fundamentos del análisis de varianza (ANOVA), el concepto formal de hipótesis nula, el p-valor como criterio de contraste, y dio forma a los diseños experimentales aleatorios. Su libro *Statistical Methods for Research Workers* fue una revolución. Trabajó con datos agrícolas, genéticos, y publicó en múltiples disciplinas. A pesar que fue un enemigo acérrimo de Pearson en muchos asuntos, también defendió ideas eugenésicas y escribió textos lisa y llanamente racistas. Además defendió la imagen pública de las tabaqueras (de las cuales recibió cuantiosos aportes monetarios). Algunos de sus aportes lo sitúan como uno de los matemáticos más brillantes e importantes de la historia, pero muchas de sus intervenciones lo evidencian como un racista nocivo y propagador de desinformación y pseudociencia. Durante su vida, después también fue rival de **Egon Pearson** (hijo de Karl), quién junto con **Jerzy Neyman**, desarrolló un marco alternativo para la inferencia estadística: el enfoque de prueba de hipótesis con control de error tipo I y tipo II. El famoso “contraste Neyman-Pearson”.

En un registro muy distinto, mientras otros se debatían entre fórmulas y supremacías raciales, **Florence Nightingale** (1820–1910), *La Dama de la Lámpara* recorría de noche los hospitales cuidando y chequeando a pacientes y aplicaba estadísticas para salvar vidas. En medio de la Guerra de Crimea, documentó de manera meticulosa las causas de muerte de soldados, y descubrió que la mayoría no moría por heridas de batalla, sino por infecciones evitables. Diseñó gráficos circulares impactantes -hoy conocidos como *gráficos de área polar*- que persuadieron a políticos y militares de cambiar la organización hospitalaria. Fue pionera en combinar ciencia, comunicación y política pública.

Décadas más tarde, otro personaje poco convencional, **John Tukey** (1915–2000), marcaría una inflexión. Trabajando en Bell Labs y luego en Princeton, impulsó el análisis exploratorio de datos: una idea sencilla pero poderosa. En vez de partir con hipótesis rígidas, propuso mirar los datos con atención, graficar, buscar patrones inesperados, dejar que los datos hablen. Inventó herramientas como el **boxplot o gráfico de cajas**, defendió el uso de gráficos como método analítico legítimo, y fue uno de los primeros en ver venir el problema de los volúmenes masivos de datos.

Esa visión se haría realidad con el paso al análisis computacional. A mediados del siglo XX, el desarrollo de computadores abrió posibilidades antes impensadas. **Ada Lovelace** (1815–1852), varias décadas antes, ya había imaginado que una máquina podía ejecutar instrucciones para procesar información. Y **Alan Turing** (1912–1954), desde su trabajo en criptografía y lógica, establecería los fundamentos de la computación moderna. Inventó la idea teórica de una “máquina de Turing”, capaz de ejecutar cualquier algoritmo con reglas básicas. Durante la Segunda Guerra Mundial ayudó a descifrar el código nazi Enigma, lo que probablemente acortó la guerra y salvó millones de vidas. En vez de recibir un reconocimiento, fue perseguido por su orientación sexual, condenado a castración química. Murió en un aparente suicidio, en 1954 tras ingerir una manzana envenenada con cianuro. Una de sus historias favoritas era Blancanieves.

Durante el siglo XX, la estadística se expandió a nuevos territorios. En Estados Unidos, **Gertrude Cox** (1900–1978) fue una figura clave en el desarrollo de diseños experimentales aplicados, particularmente en agricultura, y fue la primera mujer en dirigir un departamento de estadística en una universidad estadounidense. También impulsó la creación del Instituto de Estadística de Carolina del Norte, ayudando a consolidar la estadística aplicada como campo académico autónomo y con fuerte orientación práctica. Su trabajo no fue teórico en abstracto: fue estadística con tierra en las botas. Más recientemente, **Joan Garfield** (n. 1946) ha sido una voz fundamental en el ámbito de la educación estadística. Investigó cómo aprenden los estudiantes conceptos como variabilidad, distribución y probabilidad, y promovió una visión de la enseñanza basada en la comprensión conceptual, más allá de la memorización de fórmulas. Sus propuestas han influido programas de formación docente en todo el mundo, y han hecho que pensar en enseñar estadística no sea sólo cosa de tirar ejercicios, sino de cultivar pensamiento crítico.

En paralelo, florecieron ramas específicas de la estadística vinculadas a la medición psicológica y social. Figuras como **Charles Spearman** (1863–1945), con su teoría del factor g y el coeficiente de correlación de rangos; **Louis Thurstone** (1887–1955), pionero en análisis factorial y en métodos de escalamiento; **Rensis Likert** (1903–1981), creador de las escalas tipo Likert tan omnipresentes en encuestas sociales; o **Lee Cronbach** (1916–2001), con su famoso alfa para estimar confiabilidad, ayudaron a consolidar lo que hoy llamamos psicometría. Y aunque a veces esa tradición se caricaturiza como excesivamente técnica, fue también un esfuerzo por construir instrumentos válidos, confiables y significativos para estudiar fenómenos complejos.

En Latinoamérica, aunque muchas veces marginados del relato dominante, hubo desarrollos importantes. A lo largo del siglo XIX y XX se organizaron censos nacionales, se crearon institutos de estadística, y se desarrollaron sistemas de información que permitieron monitorear fenómenos sociales y de salud pública a gran escala. En Chile, por ejemplo, el **Instituto Nacional de Estadísticas (INE)** fue fundado en 1843, y ha sido responsable de organizar censos de población y vivienda, encuestas de empleo, precios, consumo, y más. Pero no actúa solo: numerosas otras instancias gubernamentales producen o dependen de estadísticas, como la **Encuesta CASEN**, la **Encuesta Nacional de la Primera Infancia**, la **Encuesta del Uso del Tiempo**, y muchas otras. En principio, todo esto debiera servir para diseñar políticas públicas basadas en evidencia... aunque en la práctica eso no siempre ocurre de manera rigurosa ni transparente. Pero es el marco en el que se discute, se negocia y se decide.

Desde los años 90 en adelante, el discurso sobre “evidencia para la política” ha tomado fuerza, a veces con honestidad, otras con maquillaje. Pero en ambos casos, la estadística se volvió parte del lenguaje obligado del poder público. A veces bien usada, a veces mal, siempre manoseada. Pero ahí está.

##### Software

El desarrollo de la estadística ha ido de la mano con el avance de herramientas computacionales. A comienzos del siglo XX, se hacían cálculos a mano o con rudimentarias calculadoras mecánicas. En la segunda mitad del siglo, emergieron los primeros softwares diseñados para analizar datos. **SPSS** (Statistical Package for the Social Sciences), lanzado en 1968, fue pionero en permitir que investigadores de ciencias sociales sin formación matemática profunda pudieran ejecutar análisis estadísticos con comandos relativamente simples. **SAS**, en paralelo, se orientó más hacia el mundo empresarial y de salud.

**Excel**, nacido como parte de Microsoft Office en los 80, no fue diseñado específicamente para hacer estadística, pero se volvió ubicuo en oficinas, universidades y consultoras. Su interfaz de planilla y su baja barrera de entrada lo hicieron casi omnipresente. Claro que con gran poder viene gran irresponsabilidad: Excel es famoso por errores de fórmula, columnas mal pegadas, y decisiones tomadas a partir de promedios sin contexto. Pero sigue siendo una herramienta, que bien usada puede ser útil, y es puerta de entrada para muchas personas.

A fines del siglo XX aparece **R** (desarrollado por Ross Ihaka y Robert Gentleman en los años 90), un lenguaje libre, potente y extensible, basado en el lenguaje S del Bell Labs. R no solo permitió hacer estadística avanzada sin depender de licencias costosas, sino que promovió un cambio de paradigma: la idea de que el análisis debía ser reproducible, documentado y compartido. Con R y su ecosistema -RStudio, paquetes como *tidyverse*, Markdown, Shiny, etc.- se abrió una nueva etapa. Hoy, R es la herramienta de referencia para análisis en múltiples disciplinas, especialmente en ciencia abierta, publicaciones reproducibles y enseñanza avanzada.

Junto a él, también crecieron herramientas como **Python** (más generalista, pero con librerías estadísticas y de aprendizaje automático como *pandas*, *numpy*, *scikit-learn*), **Jamovi**, **JASP**, y entornos como **Stata** y **Mplus** para modelado más especializado. El punto no es cuál es mejor. El punto es que el software no es solo una calculadora, sino que es parte del proceso científico. Y elegir bien -entender sus supuestos, su transparencia, sus limitaciones- también es parte del trabajo.

##### Big data e inteligencia artificial

Hoy, el mundo está inundado de datos. No vivimos un déficit de información, sino una sobrecarga de señales cruzadas, algoritmos opacos, dashboards infinitos. La estadística está en todas partes: en redes sociales, marketing, diagnósticos médicos, modelos climáticos, estudios de opinión, inteligencia artificial. Los llamados modelos de lenguaje (LLM), como ChatGPT o Claude, existen gracias a métodos estadísticos que permiten detectar patrones, predecir palabras, ajustar pesos en redes neuronales.

La historia de estos modelos comienza con el **perceptrón** de **Frank Rosenblatt** (1958), un intento temprano de simular el aprendizaje. Décadas después, **Geoffrey Hinton** -psicólogo de formación, y luego una de las figuras centrales del aprendizaje profundo (*deep learning*) - retomó estas ideas y ayudó a desarrollar las arquitecturas que hoy dan vida a sistemas como ChatGPT. Ganó el Premio Turing y, en 2024, fue uno de los galardonados con el Nobel de Física por sus contribuciones. Su motivación original: entender cómo funciona el cerebro humano.

Es interesante cerrar con esto. Porque aunque el *deep learning* parece muy lejos de la estadística descriptiva, en realidad comparte su alma: la búsqueda de patrones, la sistematización de observaciones, la construcción de modelos a partir de datos. Solo que ahora a una escala inmensa y con implicancias que apenas empezamos a entender.

En este contexto, la alfabetización estadística es una necesidad. Para entender el mundo, para defendernos de sus distorsiones, para construir conocimiento útil. Esta historia -hecha de avances, errores, intuiciones brillantes y decisiones cuestionables- es la base sobre la cual se apoya todo lo que viene después. Saber de dónde venimos no garantiza hacer bien las cosas, pero al menos permite intentar que no repitamos los errores sin saberlo.

